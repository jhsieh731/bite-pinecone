{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "from pinecone import Pinecone, ServerlessSpec, list_indexes, Index\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "APP_KEY = os.getenv(\"APP_KEY\")\n",
    "APP_SECRET = os.getenv(\"APP_SECRET\")\n",
    "REDIRECT_URI = os.getenv(\"REDIRECT_URI\")\n",
    "\n",
    "\n",
    "auth_flow = dropbox.DropboxOAuth2FlowNoRedirect(APP_KEY, APP_SECRET, locale=REDIRECT_URI, token_access_type='offline')\n",
    "# Get the authorization URL\n",
    "authorize_url = auth_flow.start()\n",
    "print(\"1. Go to: \" + authorize_url)\n",
    "print(\"2. Click 'Allow' (you might have to log in first).\")\n",
    "print(\"3. Copy the authorization code, paste it in next block\")\n",
    "\n",
    "auth_flow.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange the authorization code for an access token and refresh token\n",
    "oauth_result = auth_flow.finish(os.getenv(\"AUTHORIZATION_CODE\"))\n",
    "print(\"Access token:\", oauth_result.access_token)\n",
    "print(\"Refresh token:\", oauth_result.refresh_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_KEY = os.getenv(\"PINECONE_KEY\")\n",
    "DROPBOX_KEY = os.getenv(\"DROPBOX_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_KEY)\n",
    "dbx = dropbox.Dropbox(app_key=APP_KEY, app_secret=APP_SECRET, oauth2_refresh_token=oauth_result.refresh_token)\n",
    "\n",
    "\n",
    "SHOWS = dbx.files_list_folder('/Apps/Basketball Podcast Transcript').entries\n",
    "NUM_SHOWS = len(SHOWS)\n",
    "show = SHOWS[0].path_display\n",
    "episodes = dbx.files_list_folder(show).entries\n",
    "episode = episodes[1].path_display\n",
    "\n",
    "dbx.files_list_folder(episode).entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded /Apps/Basketball Podcast Transcript/All The Smoke - 20240404-093725/All-Star Recap, Caitlin Clark, Lakers or Warriors ft Gillie Da K/All The Smoke - All-Star Recap, Caitlin Clark, Lakers or Warrior.tsv to transcripts/All The Smoke - All-Star Recap, Caitlin Clark, Lakers or Warrior.tsv\n"
     ]
    }
   ],
   "source": [
    "def list_tsv_files(folder_path):\n",
    "    try:\n",
    "        # List files in the specified folder\n",
    "        response = dbx.files_list_folder(folder_path)\n",
    "        # Filter for .tsv files\n",
    "        tsv_files = [entry for entry in response.entries if isinstance(entry, dropbox.files.FileMetadata) and entry.name.endswith('.tsv')]\n",
    "        return tsv_files\n",
    "    except dropbox.exceptions.ApiError as err:\n",
    "        print('Failed to list .tsv files:', err)\n",
    "        return []\n",
    "    \n",
    "\n",
    "def download_file(file_path, local_path):\n",
    "    try:\n",
    "        dbx.files_download_to_file(local_path, file_path)\n",
    "        print(f\"Downloaded {file_path} to {local_path}\")\n",
    "    except dropbox.exceptions.ApiError as err:\n",
    "        print(f\"Failed to download {file_path}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to encode sentences\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'transcripts' not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=\"transcripts\",\n",
    "        dimension=model.get_sentence_embedding_dimension(),\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index('transcripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check transcript not already uploaded\n",
    "def check_id_exists(id_to_check):\n",
    "    result = index.fetch(ids=[id_to_check], namespace='')\n",
    "\n",
    "    if len(result['vectors'].keys()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# upload set of vectors in batches\n",
    "def upload_in_batches(vectors, batch_size=100):\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "# remove whitespaces in id and replace special characters with _\n",
    "def clean_id(text):\n",
    "    text = text.replace(' ', '')\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", '_', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index():\n",
    "    for show in SHOWS:\n",
    "        podcast_name = show.name\n",
    "        episodes = dbx.files_list_folder(show.path_display).entries\n",
    "        num_episodes = len(episodes)\n",
    "        count = 0\n",
    "        for episode in episodes:\n",
    "            count += 1\n",
    "            episode_name = episode.name\n",
    "            files = list_tsv_files(episode.path_display)\n",
    "            for file in files:\n",
    "                filename = 'transcripts/' + file.name\n",
    "                id = clean_id(podcast_name + episode_name + str(0))\n",
    "                if check_id_exists(id):\n",
    "                    break\n",
    "                download_file(file.path_display, filename)\n",
    "                df = pd.read_csv(filename, sep='\\t', keep_default_na=False)\n",
    "                df['vector'] = df['text'].apply(lambda x: model.encode(str(x)))\n",
    "                upload_data = [{\n",
    "                    'id': clean_id(podcast_name + episode_name + str(i)),\n",
    "                    'values': row.vector.tolist(),\n",
    "                    'metadata': {\n",
    "                        'start_time': row.start,\n",
    "                        'stop_time': row.end,\n",
    "                        'podcast': podcast_name,\n",
    "                        'episode': episode_name,\n",
    "                        'text': row.text,\n",
    "                    },\n",
    "                    } for i, row in df.iterrows()]\n",
    "                upload_in_batches(upload_data)\n",
    "                os.remove(filename)\n",
    "                break\n",
    "            print(f\"{count}/{num_episodes}\")\n",
    "# create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize helpers & index from scraper portion (assuming data all loaded)\n",
    "\n",
    "# model to encode sentences\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "PINECONE_KEY = os.getenv(\"PINECONE_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_KEY)\n",
    "\n",
    "# remove whitespaces in id and replace special characters with _\n",
    "def clean_id(text):\n",
    "    text = text.replace(' ', '')\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", '_', text)\n",
    "    return text\n",
    "index = pc.Index('transcripts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_start_end_sims(start_i, end_i, clip, clip_keys, num_clips, clips):\n",
    "        start_sim = end_sim = -1\n",
    "        start_v = np.array(clips[clip_keys[start_i]].values).reshape(1, -1)\n",
    "        end_v = np.array(clips[clip_keys[end_i]].values).reshape(1, -1)\n",
    "\n",
    "        if (start_i - 1) >= 0:\n",
    "            prev_v = np.array(clips[clip_keys[start_i - 1]].values).reshape(1, -1)\n",
    "            start_sim = cosine_similarity(start_v, prev_v)\n",
    "        if (end_i + 1) < num_clips:\n",
    "            next_v = np.array(clips[clip_keys[end_i + 1]].values).reshape(1, -1)\n",
    "            end_sim = cosine_similarity(end_v, next_v)\n",
    "        return start_sim, end_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"results\": [{\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Lakers beat Suns, Under Duress List, Refs the biggest reason Mav\", \"start_time\": 564000.0, \"start_clip_end\": 565000.0, \"end_time\": 671000.0}, {\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Lamar Jackson continues to Tweet, Warriors comeback against Peli\", \"start_time\": 949000.0, \"start_clip_end\": 953000.0, \"end_time\": 1044000.0}, {\"show\": \"The Mark Titus Show - 20240404-093622\", \"episode\": \"Episode 18 Draymond Green SUSPENDED For Stomping on Domantas Sab\", \"start_time\": 1394520.0, \"start_clip_end\": 1398400.0, \"end_time\": 1608460.0}, {\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Lamar Jackson continues to Tweet, Warriors comeback against Peli\", \"start_time\": 10000.0, \"start_clip_end\": 13000.0, \"end_time\": 88000.0}, {\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Aaron Rodgers and Lamar Jackson news, Raiders free agent signing\", \"start_time\": 64000.0, \"start_clip_end\": 67000.0, \"end_time\": 132000.0}, {\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Doc Rivers calls Embiid the MVP, Nick\\'s Playoff Player Pyramid,\", \"start_time\": 1959160.0, \"start_clip_end\": 1962280.0, \"end_time\": 1987640.0}, {\"show\": \"The Athletic NBA Show - 20240404-094004\", \"episode\": \"Big Market Dependence with Ethan Strauss\", \"start_time\": 1743440.0, \"start_clip_end\": 1748960.0, \"end_time\": 2000000.0}, {\"show\": \"First Things First - 20240404-093806\", \"episode\": \"Embiid\\'s flagrant foul, Warriors win without Draymond, How will\", \"start_time\": 1501460.0, \"start_clip_end\": 1504940.0, \"end_time\": 1535420.0}, {\"show\": \"All The Smoke - 20240404-093725\", \"episode\": \"Steph vs Lebron Rivalry Renewed, Dillon Brooks, Knicks  WHAT\\'S B\", \"start_time\": 1081840.0, \"start_clip_end\": 1084760.0, \"end_time\": 1127160.0}, {\"show\": \"The Athletic NBA Show - 20240404-094004\", \"episode\": \"The Gritty Lakers, Trading CJ McCollum, and Warriors Deep Dive w\", \"start_time\": 223000.0, \"start_clip_end\": 225000.0, \"end_time\": 248000.0}]}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MIN_LEN = 1000 * 20\n",
    "MAX_LEN = 1000 * 300\n",
    "SCAN_RANGE = 40\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "def search(query):\n",
    "    query_vector = model.encode(query)\n",
    "    results = index.query(vector=query_vector.tolist(), top_k=10, include_metadata=True)['matches']\n",
    "    filtered_results = [match for match in results if match['score'] >= THRESHOLD]\n",
    "    json_res = []\n",
    "    for result in filtered_results:\n",
    "        id_name = clean_id(result['metadata']['podcast'] + result['metadata']['episode'])\n",
    "        id_num = int(result['id'].replace(id_name, ''))\n",
    "        # print(result['id'], id_num)\n",
    "        full_clip = index.fetch(ids=[id_name + str(i) for i in range(id_num-SCAN_RANGE, id_num+SCAN_RANGE)])\n",
    "        clips = full_clip['vectors']\n",
    "        clip_keys = np.sort(list(clips.keys()))\n",
    "        num_clips = len(clip_keys)\n",
    "        start_i, end_i = SCAN_RANGE, SCAN_RANGE\n",
    "        min_sim = float('inf')\n",
    "        \n",
    "        # build the clip until it reaches the minimum length\n",
    "        while (clips[clip_keys[end_i]].metadata['stop_time'] - clips[clip_keys[start_i]].metadata['start_time']) < MIN_LEN:\n",
    "            start_sim, end_sim = find_start_end_sims(start_i, end_i, clips[clip_keys[start_i]], clip_keys, num_clips, clips)\n",
    "            min_sim = min(min_sim, start_sim, end_sim)\n",
    "            if start_sim >= end_sim:\n",
    "                start_i -= 1\n",
    "            else:\n",
    "                end_i += 1\n",
    "\n",
    "        # expand the clip until it reaches the maximum length or the similarity is below the threshold\n",
    "        while (clips[clip_keys[end_i]].metadata['stop_time'] - clips[clip_keys[start_i]].metadata['start_time']) < MAX_LEN:\n",
    "            start_sim, end_sim = find_start_end_sims(start_i, end_i, clips[clip_keys[start_i]], clip_keys, num_clips, clips)\n",
    "\n",
    "            if max(start_sim, end_sim) < min_sim:\n",
    "                break\n",
    "            if start_sim > end_sim:\n",
    "                start_i -= 1\n",
    "            else:\n",
    "                end_i += 1\n",
    "        res = {'show': result['metadata']['podcast'],\n",
    "               'episode': result['metadata']['episode'],\n",
    "               'start_time': clips[clip_keys[start_i]].metadata['start_time'],\n",
    "               'start_clip_end': clips[clip_keys[start_i]].metadata['stop_time'],\n",
    "               'end_time': clips[clip_keys[end_i]].metadata['stop_time']}\n",
    "        json_res.append(res)\n",
    "\n",
    "        # # DEBUG: print the clip transcript\n",
    "        # print('============ INDICES', start_i, end_i)\n",
    "        # for clip in clip_keys[start_i:end_i+1]:\n",
    "        #     text = clips[clip].metadata['text'] + ' '\n",
    "        #     print(text)\n",
    "\n",
    "    \n",
    "    return json.dumps({'results': json_res})\n",
    "\n",
    "search(\"warriors winning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
